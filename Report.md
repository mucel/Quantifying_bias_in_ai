# Report 
A 4-page, double-column PDF report (4 pages excluding references), following a standard structure (where applicable): abstract, introduction, related work, (brief) data collection, dataset description with summary statistics, methods with math and description of main algorithms, results and findings, conclusions. This report will be evaluated according to how clearly and succinctly it is written, if the style is appropriate (e.g., figures with captions) if it contains all relevant content, and how solid the results are.

# Quantifying the differences in bias in ChatGPT between Latvian and English. 
By Māra Učelniece, Ralfs Brutāns, Michalina Loch.

## Abstract (300 words, but is not part of word count!)

Large Language Models are known for perpetuating (if not – amplifying) the prejudices existing in society, yet still not enough has been done in the direction of bias prevention. Algorithms are being deployed in each sector of life, from high-impact fields such as healthcare, finance and immigration, to everyday matters such as retail and services, at a speed that often does not allow for extensive evaluation. This issue is likely to be especially critical in languages that do not receive a lot of attention by both software developers and researchers - such as smaller-scale languages, already at a disadvantage due to the limited amount of training data available.

This research will address the stereotypical biases exhibited by one of the most popular language models - ChatGPT, with the aim of quantifying the difference between two languages – English and Latvian. We have chosen these languages in particular, due to the enormous discrepancy in training data - English being the primary language of most (if not all) popular LLMs, while Latvian being almost entirely excluded from them. Furthermore, 2 of our group members are native Latvian speakers, therefore they have the necessary insight and language skills to oversee the translation and comparison. The method we have chosen for our study is prompt probing, following the example of Tamkin et al. (2023), thanks to its feasibility within our time and skillset. We have attempted to analyse ChatGPT-3.5 turbo 0125 with prompts acquired from Tamkin et al., testing both explicit and implicit bias in binary decision tasks.

## Introduction (500 words is 300 atm)

Large Language Models (LLMs) are becoming increasingly used in all areas of life, oftentimes authorized to guide high-risk decisions that impact people's livelihoods. This has led to a rise in the amount of research focused towards detecting and mitigating biases that occur in AI algorithms (Arrieta et al., 2019). At the moment, there are various developed methods that investigate the interoperability and explainability to assess the fairness (in this research defined as balanced treatment of various communities and individuals) in the decision-making cycle of algorithms (Suresh & Guttag, 2021). This research utilizes the method called prompt probing, in which one investigates the behavior of LLMs by using carefully constructed prompts and examining the generated outputs. This is the method utilized by Tamkin et al. in 2023, whose study we aim to replicate on a different model (ChatGPT 3.5 instead of Claude 2.0) and contrasting the outcomes of two languages (English and Latvian) side by side. Hence, our study will be guided by a research question: To what extent ChatGPT 3.5 turbo 0125 exhibits a difference in the level of prejudice when confronted with prompts in Latvian versus English? Within this aim, we have chosen to use the open-source database of prompts created by Tamsin et al. due to their extensive process of assessment and validation of wording and neutrality of each prompt, which is a fundamental base for obtaining meaningful results. The reason for selecting ChatGPT 3.5 turbo as our model was based on two main criteria. First, the model had to be trained and available in Latvian, which most models are disappointingly not. Secondly, according to previous research, ChatGPT 3.5 had already exhibited better performance scores than competitors at its time and has been widely used by the public (Sun et al., 2023), therefore its shortcomings might be most impactful to society. This will be done by going into depth on current research that influenced the selected method. Explain the method that the replicated study used, to highlight the aspects that are incorporated in this research. A brief summary of the use of APIs in the context of prompt probing. Then the results and their analysis, after which the limitations and possible solutions will be addressed. Additionally, this research will contribute to the broader discourse on AI fairness and ethics, highlighting the importance of multilingual capabilities in LLMs and the potential societal impacts of their biases. Through this approach, we aim to offer insights that could inform the development of more equitable AI systems in the future.

## Theoretical background 

Based on previous research, we have found that surface-level performance evaluations of LLMs from both systems and user perspective is frequently done through the usage of datasets, from both systems and users perspective. Alongside datasets, numerous new evaluation metrics have been constructed, with an aim to assess the biases and overall behaviour exhibited by the models. One interesting example encountered in literature was the reserach of SterioSet, in which they created an idealized score to compared a given model to an idealistic model (Sun et al., 2023, Kotek et al., 2023, Smith et al., 2022, Nadeem et al., 2020). However, some research mainly focused on  using these newly created metrics for the evaluation of performance (Sun et al., 2023) rather than the quantification of bias. Many that did attemp to quantify biase focused on one singular one: gender (Kotek et al., 2023). A compelling novel method that was not rooted in dataset usage was a model-free approach, that entails probing used as a prompting task, in the hopes of analysing responses without leveraging any specific knowledge (Li et al., 2022). Unfortunately, this is primarily useful when identifying embedded linguistic properties, and is less less appropriate for our research aim. Another relatively new approach is the unanticipated bias detection, through the use of Uncertainty Quantification and Explainable AI methods, that allow for detection of less obvious, implicit biases (Kruspe, 2024). Still, that work mainly focuses on how explainability can help the users identify bias. As our intent was to explore multiple demographic biases: gender, age and race, we have settled on prompt probing, an approach popularly used for testing a models factual knowledge retrieval (Jiang et al., 2020, Brown et al., 2020, Zhong et al., 2021). This method was deemed fitting as it allows for detailed examination in various contexts and scenarios, usage of multiple demographic signifiers and straight-forward statistical analysis of results. Thus, the method allowed us to attempt at simulating real-world relevance through hypothetical scenarios, providing insight into sociocultural implications. 

The main research that was chosen for replication of prompts and methods for evaluating and quantifying discriminatory outputs from LMs was "Evaluating and Mitigating Discrimination in Language Model Decisions" by Tamkin et al. (2023). Their work focused on identifying and mitigating bias using English prompts for various decision-making scenarios in the Claude 2.0 model. This study seeks to recreate and extend their research by assessing Chat-GPT3 turbo on the differences in exhibited biases between English and Latvian. 

Especially when taking into consideration that the global population of Latvian speakers is less than 2 million, in contrast to an estimated 1.45 billion English speakers worldwide (Latviešu Valoda, n.d., WordsRated, 2023). Along with the fact that any AI model's performance is related to the amount of data that has been used to train them, and that in English these models have been equipped with a significantly larger corpus (Lucchi, 2023, Taulli, 2023). As a result, they exhibit superior performance when processing English prompts, while smaller-scale languages remain significantly more prone to amplifying cultural stereotypes. Understanding bias in multilingual LMs is crucial for ensuring fair and ethical applications across diverse languages and directing efforts to mitigate and prevent discrimination from becoming codified with the increased use of algorithms across various industries.


##  Methodology 

### Experimental setup
We utilize the existing dataset of prompts developed by Tamkin et al. (2023), encompassing over 90 diverse topic areas for realistic decision-making scenarios. Each prompt has been constructed through a structure template that contains placeholders for varying demographic information (e.g., name, age, location) so that there were no differences in wording that could affect the model's answers. These prompts have been assessed by a human evaluation study and achieved high ratings on quality. For this reason, we decided to use this database instead of preparing our own, as creating objective, neutral and non-leading prompts would have been impossible with our limited resources and prompts free of any suggestion are the core basis for getting meaningful results, and thus also a meaningful evaluation of bias in a model. 
This basis only consisted of prompts in English, as Tamkin et al. did not compare the results between other languages, therefore we needed to create the Latvian equivalent of each prompt. Ergo, they have been translated into grammatically accurate, semantically and syntactically analogous Latvian by using Google API, and then cross-evaluated by two native Latvian speakers to ensure a reliable and comparable translation. In order to achieve meaningful results, our experiment required a large volume of prompts, making it impossible to translate by hand. Furthermore, manual translation could give way to human error and inconsistency across prompts, therefore we have decided that Google API combined with manual validation would yield the most reliable results. The resulting dataset included 37800 prompts (18900 in English, 18900 in Latvian) covering a range of decision-making domains, both high-risk (like loan approvals, employment opportunities, and criminal justice) and low-risk (such as approving a merchandise return). The prompts are organized into separate files for explicit demographic information (such as "The patient is a 20-year-old Asian female suffering from (...)") and implicit signifiers ("Ling Kim is a 40-year-old person suffering from (...)"). We have also added an instruction at the end of each prompt, to only output a yes/no decision (with "yes" signifying the favourable outcome in each hypothetical scenario). 

The primary algorithm used will be the built-in processing capabilities of Chat GPT-3.5 Turbo. We will not be modifying the internal algorithms of the model itself or tweaking the model since our goal of the experiment is to investigate bias in an already existing and deployed large language model. 
Each prompt (English and Latvian) has been fed individually to a new session of ChatGPT 3.5 Turbo in order to prevent the cross-influence of prompts in one session. In order to feed the prompts into the model we have used OpenAI's offered paid API service which granted us the ability to automate the process.
The results have been collected within a new column added to each JSON Lines file (containing the prompt dataset), and labelled as "answer". That column was then transformed into binary notation (1-positive answer, 0-negative answer, 2-for unclear, ambiguous outputs) in a column labelled "bool" in order to streamline the further calculations. 


### Analysis of the results
In order to interpret the results we will calculate a discrimination score metric, as outlined in the original study. This score quantifies the degree of bias exhibited by the model's decisions based on demographic variations within the prompts. For that, it is necessary to establish a reference point with which to compare how different demographic profiles are treated. Tamkin et al. (2023) used a 60-year-old white male as the baseline "applicant" in their study, due to historical privilege, and we decided to follow such a choice for the sake of comparability. For each demographic variation within a prompt, we will calculate two key differences – 
+ Positive Decision Difference (P_pos(+)) – the extent to which applicants of a specific demographic are more likely (positive difference) or less likely (negative difference) to receive a "yes" outcome compared to the baseline applicant.
+ Negative Decision Difference (P_neg(+)) - equivalent as above, for a "no" outcome.
The discrimination score for a specific demographic attribute within a prompt is calculated as the average of the two aforementioned differences:
$$D = (|logit[pnorm(yes)_+] - logit[pnorm(yes)_-]| + |P_neg(+) - P_neg(-)|) / 2$$

where:
* D = Discrimination score
* $logit[pnorm(yes)_+]$ = Logit transformed probability of a positive decision for applicants with the demographic attribute
* $logit[pnorm(yes)_-]$ = Logit transformed probability of a positive decision for the baseline applicant
* $P_neg(+)$ = Negative decision rate for applicants with the demographic attribute
* $P_neg(-)$ = Negative decision rate for the baseline applicant

The logit function transforms probabilities (between 0 and 1) into a more interpretable scale, and thus improves the normality of the data for statistical analysis. 


## Results 

#### Explicit Bias Analysis

##### Race Discrimination Scores:

In the explicit bias analysis, examining race discrimination scores across languages reveals interesting patterns. When comparing English and Latvian prompts, it's evident that Latvian prompts generally exhibit lower discrimination scores compared to English prompts. For instance, the discrimination score for the ('white', 'Black') race pair in English prompts is 0.0190, while in Latvian prompts, it reduces to 0.0085, indicating less bias in Latvian prompts regarding this racial pairing. Similar trends are observed across other racial pairings, with Latvian prompts consistently showing lower discrimination scores.

##### Gender Discrimination Scores:

Gender discrimination scores also display variations between English and Latvian prompts. In the case of ('female', 'male') gender pairing, English prompts demonstrate a discrimination score of 0.0012, whereas Latvian prompts show a slightly higher score of 0.0111. However, when comparing other gender pairings like ('female', 'non-binary') and ('male', 'non-binary'), English prompts consistently exhibit higher discrimination scores compared to Latvian prompts.

##### Age Discrimination Scores:

Age discrimination scores illustrate nuanced differences between English and Latvian prompts. While some age brackets show similar discrimination scores across both languages, such as (20.0, 30.0) and (40.0, 50.0), others display notable variations. For instance, the discrimination score for the (30.0, 60.0) age bracket is 0.0352 in Latvian prompts, significantly higher than the score of 0.0038 in English prompts, indicating a higher level of bias in Latvian prompts for this age range.

#### Implicit Bias Analysis

##### Race Discrimination Scores:

Implicit bias analysis also showcases differences between English and Latvian prompts, albeit with some variations in the racial pairings. For instance, while the ('white', 'Black') racial pairing in English prompts has a discrimination score of 0.0021, the same pairing in Latvian prompts shows a higher score of 0.0159, indicating more implicit bias in Latvian prompts regarding this racial pairing. However, other racial pairings exhibit lower discrimination scores in Latvian prompts compared to English prompts.

##### Gender Discrimination Scores:

Similarly, gender discrimination scores in implicit bias analysis reveal contrasting trends between English and Latvian prompts. While some gender pairings, like ('female', 'male'), display lower discrimination scores in English prompts compared to Latvian prompts, others show the opposite pattern, with Latvian prompts exhibiting lower discrimination scores.

##### Age Discrimination Scores:

Age discrimination scores also demonstrate variations between English and Latvian prompts, with certain age brackets displaying higher discrimination scores in Latvian prompts compared to English prompts, and vice versa.

Overall, the comparison of explicit and implicit bias analysis between English and Latvian prompts indicates varying levels of bias across different demographic categories, underscoring the importance of considering linguistic and cultural nuances when evaluating bias in language models. Further analysis and mitigation strategies may be warranted to address these disparities and promote fairness and equity in AI applications across diverse languages.

### Evaluation of the results (400 words)
![Explicit English Plot](exp_eng_plot.png)
Figure 1. Answers for each of the categories for explicit prompts in English. Each of these plots indicates the amount of no answer/no/yes answer given by the model when a prompt contained said trait.
![Implicit English Plot](imp_eng_plot.png)
Figure 2. Answers for each of the categories for implicit prompts in English.
![Explicit Latvian Plot](exp_lv_plot.png)
Figure 3. Answers for each of the categories for explicit prompts in Latvian.
![Implicit Latvian Plot](imp_eng_plot.png)
Figure 4. Answers for each of the categories for implicit prompts in Latvian.

       After summarizing the data and creating plots that indicate the amount of yes/no answers that were given by the model for each of the prompts, it is clearly visible that there exists a clear "yes" bias within the model. The bias is most explicit in the English implementation of the model, both when answering implicit and explicit prompts. For explicit Latvian prompts there is a noticeable increase in the amount of "no" answers received. This could be explained by the model having a smaller "yes" bias when explicit attributes are provided for training data in Latvian. Despite this high difference in "no" answers between the two languages, there does not appear to be an increase in bias. Moreover, inspecting these plots also reveals that there is not a clear pattern of discrimination visible in both Latvian and English answers from ChatGPT 3.5 Turbo. More specifically, there exists a varying amount of yes and no answers for each of the categories in both languages, but no visible patterns emerge when comparing implicit and explicit prompts within one language.
      When comparing figure 1 and figure 2 it is visible that the differences between explicit and implicit prompts in English there is only a slight difference between the amount of yes/no answers and the differences are minute and that there exists a discrepancy between the amount of yes/no answers within different categories implying that there does not exist a clear bias in the English implementationn of ChatGPT 3.5 Turbo.
      Furthermore, in figure 3 and figure 4 it is visible that there does exist a clear discrepancy between the amount of yes/no answers between implicit Latvian and explicit Latvian prompts. The amount of yes answers in implicit Latvian is noticeably higher when compared to the answers to explicit Latvian prompts, comparable to the distribution of yes/no answers in English answers. However, similiarly to the English answers it does not seem that the modelexhibits a clear pattern of discrimination for any of the categories, because the differences between categories are different between explicit and implicit prompts.
      To sum up, the model does not seem to exhibit a clear bias towards any specific trait in both languages. The model clearly demonstrates a "yes" bias for these types of questions, however less so for explicit Latvian prompts. When comparing the answer distribution within one language, there are differences between the answer distributions indicating that age, race or gender bias is not prominent in this language model.


## Discussion (750 words is 714 atm)

The primary contribution of this paper was to highlight the biases that can occur even in closely related models, while not denying the fact that various languages do not have equivalent accuracy scores, which OpneAI acknowledged themselves (OpenAI, 2023). Our analysis focused on the various biases in explicit and implicit prompt probing. That was analysed based on discrimination scores of each group. The results reflected that there is a clear "yes" bias in the outputs, while with Latvian this was less prevalent, which indicates that there are linguistic social disparities rooted in the LLMs. However, there were many factors that influenced the process and evaluation of the study.  


### Limitations 
A key limitation that was faced during this research related to the intent to explore a lesser-spoken language - latvian, which has less than 2 million fluent speakers worldwide (Latviešu Valoda, n.d.). This led to our most considerable restriction in the model selection process as most models that were found were simply not trained in this language. Thus, this research refrained from open-source models as none that were found on Hugging Face and Keggle were equipped to answer the intended prompts. The focus was shifted to one of the most known and largest models trained on numerous languages - ChatGPT, which included latvian and obviously english (Funelas, 31 C.E.). Further the explicit selection of using ChatGPT3.5 turbo was based on the companies pay walls for using their api's and the amount of resources that the authors were willing to invest themselves. As the most optimal choice would have been to use the latest version ChatGPT4, notwitstanding that it was more expensive. Based on jurnalistic evedience ChatGPT4 was 60 times more expensive for input and 40 times more expensive for output (Kelly, 2024). 

The amount of resources also influenced the evaluation of the prompts. As now it was only done by researchers of the study, who are bachelor students that are native speakers, but are not specialists in linguistics in any way. Thus there could have been mistakes that went unnoticed, without taking into account various dialects that the language has. 

Another limitation was the amount of time that was allocated to this research as this was a project for a Text Mining course, around a month was given to complete it. Thus, the choice of only analysisng a one set of prompts could have lead to biases such as Prompt Preference bias, Instance Verbalization bias and Sample Disparity bias in the analysis (Cao et al., 2022), because LLMs are generally sensitive to subtle changes in the luigistic preferences expressed through wording, verbalization etc. of prompts (Jiang et al., 2020). Further research could explore simpler methods, which have been proposed to universalise the best probing criteria, when investigating factual knowledge probing (Cao et al., 2022). 


### Solutions 
The method that was used in this research: prompt probing, had some downfalls as it is only useful to identify embedded linguistic properties, thus it is a surface-level analysis that misses propagated biases in the models and would be insufficient for causality analysis as it lacks the understanding of why and how biases occur (Li et al., 2022). A proposed way to resolve this is by using a combination of methods when analysing biases (Narain, 2023). Having said that, the only way to understand completely where biases originate from one would need to assess every aspect of the Problem, Plan, Data, Analysis, Conclusion cycle of AI as biases can be introduced at almost any step (Gao & Mavris, 2022). 

Nonetheless, the evaluation of the results can also be influenced by biases of the method. A research that addresses this is "Can Prompt Probe Pretrained Language Models? Understanding the Invisible Risks from a Causal View" by Cao et al. They explained that prompt probing contains biases mainly based on the specific types of prompts, while also emphasizing the uncertainty that this method has and that it often does not even represent testing knowledge-related tasks (Cao et al., 2022). Another proposed way to mitigate negative impacts of prompt bias, specifically overfitting benchmarks and misleading language models, in factual knowledge extraction is by using the representation vector of prompt-only querying (Xu et al., 2024). Based on their findings this approach is able to rectify inflated benchmark performance while improving the retrieval capability of prompts. 



## Conclusion 
This research had used a popular method to explore the biases in LLMS, specifically in ChatGPT3.5 turbo, to see if there is a difference in between biases that are exhibited in two languages. The results reflected that there is a “yes” bias in how these LLMs answer open queries. It is important to remember that this might not be the case for other models that are not trained for producing text. The primary observation that emerged from our work is the disparity in access to the majority of large language models between languages, as most models that we tested did not support Latvian at all. This points to a bigger societal problem of equitability within the field of new technologies and who has the privilege to benefit from them. It is necessary to point out the widening gap in technological literacy, with the most heavily pertaining to the elderly. When combined with monolingualism/lack of English comprehension, which is also characteristic predominantly of older generations, this exacerbates the problem significantly and requires increased attention.


### References 
... 



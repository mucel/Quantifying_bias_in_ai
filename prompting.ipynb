{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f41de53-0bb7-4f28-a392-684b8fdd482d",
   "metadata": {},
   "source": [
    "### This is the code for prompting a LLM "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa9a96c-00ca-4739-8963-31c7401e686a",
   "metadata": {},
   "source": [
    "Would like to use chatCPT4o but can also do other models found on Hugging Face like ChatGPT-3, but as OpenAI would not provide us with a key we decided to use transformer models that can be found on the Hugging Face libaries. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0e9e92-eefa-4b73-a8f7-1c5cc87e82b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc810009d3fe48bab1700c514ccd6fc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/222 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mara\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Mara\\.cache\\huggingface\\hub\\models--bigscience--bloom. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "549c537b9b114137a4bb6a33d8aaba57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b537c5d9e8794a5caff0dc4f7303288e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfae67567c814cc085ac73d42f9dfbcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/573 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4f5691eb2e1425383746f212648a88e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/63.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f662d47a85264bfc8e6278560acc3f5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a440004f78404868afd1c5124490364d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_00001-of-00072.safetensors:   0%|          | 0.00/7.19G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import json\n",
    "\n",
    "# Loading the model from Hugging Face \n",
    "model_name = \"bigscience/bloom\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name) \n",
    "\n",
    "\n",
    "# function for reading JSONL file \n",
    "def read_jsonl(file_path): \n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file: \n",
    "        lines = file.readlines()\n",
    "        return [json.loads(line) for line in lines]\n",
    "    \n",
    "# function to send prompt to model for yes/no asnwer \n",
    "def get_response(propmt): \n",
    "    modified_prompt = prompt +\"\\nAnswer with 'yes' or 'no':\"\n",
    "    inputs = tokenizer(modified_prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs.input_ids, max_length=50)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # extracting the answer \n",
    "    if \"yes\" in response.lower():\n",
    "        return 1 \n",
    "    # the defaoult will be no\n",
    "    else: return 0 \n",
    "\n",
    "\n",
    "# reading the prompts \n",
    "data = read_jsonl(\"explicit_output.jsonl\")\n",
    "\n",
    "\n",
    "# procesing the prompts for answers\n",
    "results = []\n",
    "for item in data: \n",
    "    prompt = item[\"tamplate\"] # decision question prompt \n",
    "    respinse = get_response(propmt)\n",
    "    result.append({\n",
    "        \"prompt\": prompt, \n",
    "        \"response\": response\n",
    "    })\n",
    "    \n",
    "# saving the results to a file\n",
    "with open('responses.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# to see when it is done\n",
    "print(\"Responses have been saved to responses.json.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7decc051-bc60-4915-b5a7-af3e2c35d834",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
